 A large language model is a sophisticated mathematical function that predicts what word comes next for any piece of text . To build a chatbot, what you do is lay out some text that describes an interaction between a user and a hypothetical AI assistant . Instead of predicting one word with certainty, what it does is assign a probability to all possible next words .  Models learn how to make these predictions by processing an enormous amount of text, typically pulled from the internet . For a standard human to read the amount that was used to train GPT-3, for example, if they read non-stop 24-7, it would take over 2600 years, larger models since then, train on much more .  Large language models can have hundreds of billions of parameters . No human ever deliberately sets those parameters . Instead they begin at random, meaning the model just outputs gibberish . Algorithm called back propagation is used to tweak all of the parameters in such a way that it makes the model a little more likely to choose the true last word .  The scale of computation involved in training a large language model is mind-boggling . The goal of auto-completing a random passage of text from the internet is very different from the goal of being a good AI assistant . To address this, chatbots undergo another type of training .  Researchers at Google introduced a new language model known as the transformer . Transformers don't read text from the start to the finish, they soak it all in at once in parallel . This is only made possible by using special computer chips that are optimized for running many, many operations in parallel, known as GPUs .  Transformers typically include a second type of operation known as a feed-forward neural network . This gives the model extra capacity to store more patterns about language learned during training . All of this data repeatedly flows through many different iterations of these two fundamental operations . For example, the numbers encoding the word bank might be enriched to encode the more specific notion of a riverbank .  Researchers design framework for how each of these steps work, but it's important to understand that the specific behavior is an emergent phenomenon . This makes it incredibly challenging to determine why the model makes the exact predictions that it does . When you use large language model predictions to autocomplete a prompt, the words that it generates are uncannily fluent, fascinating, and even useful .  Aims to learn more about how transformers and attention work, here is a video of a talk on the topic . The video is a follow-up to a series I made about deep learning, where we visualize and motivate the details of attention and all the other steps in a transformer .